{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (140, 140)\n",
    "\n",
    "inception_model = InceptionV3(weights='imagenet', input_shape = target_size + (3,), include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_layer = layers.Flatten()(inception_model.output)\n",
    "dense_layer_1 = layers.Dense(512, activation='relu')(flat_layer)\n",
    "dense_layer_1 = layers.BatchNormalization()(dense_layer_1)\n",
    "dense_layer_2 = layers.Dense(256, activation='relu')(dense_layer_1)\n",
    "dense_layer_2 = layers.BatchNormalization()(dense_layer_2)\n",
    "dense_layer_3 = layers.Dense(256, activation='relu')(dense_layer_2)\n",
    "\n",
    "inception_model.trainable = False\n",
    "\n",
    "transfer_inception_model = Model(inputs = inception_model.inputs, outputs = dense_layer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityLayer(layers.Layer):\n",
    "    # compute and return the two distances:\n",
    "    # d(anchor,positive) \n",
    "    # d(anchor,negative)\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, anchor, positive, negative):\n",
    "        d1 = tf.reduce_sum(tf.square(anchor-positive), -1)\n",
    "        d2 = tf.reduce_sum(tf.square(anchor-negative), -1)\n",
    "        return(d1,d2)\n",
    "    \n",
    "anchor = layers.Input(name='anchor', shape = target_size + (3,))\n",
    "positive = layers.Input(name='positive', shape = target_size + (3,))\n",
    "negative = layers.Input(name='negative', shape = target_size + (3,))\n",
    "\n",
    "sim_layer_output = SimilarityLayer().call(\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(anchor)),\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(positive)),\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(negative))\n",
    ")\n",
    "\n",
    "siamese_model = Model(inputs=[anchor, positive,negative], outputs=sim_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModelClass(Model):\n",
    "    def __init__(self, siamese_model, margin = 0.5):\n",
    "        super(SiameseModelClass, self).__init__()\n",
    "        \n",
    "        self.siamese_model = siamese_model\n",
    "        self.margin = margin\n",
    "        \n",
    "        # create a Metric instance to track the loss\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.siamese_model(inputs)\n",
    "    \n",
    "    # customize the training process: providing our own training step\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # call custom loss function\n",
    "            loss = self.custom_loss(data)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.siamese_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update our training loss metric\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    \n",
    "    # providing our own evaluation step\n",
    "    def test_step(self, data):\n",
    "        # call custom loss function\n",
    "        loss = self.custom_loss(data)\n",
    "        \n",
    "        # Update our test loss metric\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    \n",
    "    # custom loss function\n",
    "    def custom_loss(self, data):\n",
    "        # get the distances tuple from the siamese model output\n",
    "        d1, d2 = self.siamese_model(data)\n",
    "        \n",
    "        # compute the triplet loss\n",
    "        loss = tf.maximum(d1 - d2 + self.margin, 0)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_uncompress_tarball(tarball_url, dataset_dir):\n",
    "    \"\"\"Downloads the `tarball_url` and uncompresses it locally.\n",
    "    Args:\n",
    "    tarball_url: The URL of a tarball file.\n",
    "    dataset_dir: The directory where the temporary files are stored.\n",
    "    \"\"\"\n",
    "    filename = tarball_url.split('/')[-1]\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "\n",
    "    def _progress(count, block_size, total_size):\n",
    "        sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "            filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# URL for sourcing the funneled images\n",
    "database_url = 'http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz'\n",
    "\n",
    "root_folder = '.'\n",
    "download_folder = root_folder + '/'+ 'data/lfw_original'\n",
    "selection_folder = root_folder + '/' + 'data/lfw_selection'\n",
    "download_path = download_folder + '/lfw-deepfunneled.tgz'\n",
    "\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "\n",
    "if not os.path.exists(selection_folder):\n",
    "    os.makedirs(selection_folder)\n",
    "    \n",
    "if not os.path.exists(download_path):\n",
    "    download_and_uncompress_tarball(database_url, download_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_folder = download_folder + '/lfw-deepfunneled'\n",
    "\n",
    "# images are organized into separate folders for each person\n",
    "# get a list of subfolders \n",
    "subfolders = [x[0] for x in os.walk(extracted_folder)]\n",
    "\n",
    "# first item is root the folder itself\n",
    "subfolders.pop(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list = []\n",
    "\n",
    "for path in subfolders:\n",
    "    image_count = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "    people_list.append((path.split('\\\\')[-1], image_count))\n",
    "    #people_count.append((path, image_count))\n",
    "    \n",
    "# Sort from max to min images per person\n",
    "people_list = sorted(people_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poeple list counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of people: {len(subfolders)}')\n",
    "print(f'Number of people with only one photo: {len([person for person, image_count in people_list if image_count==1])}')\n",
    "print(f'Number of people with >=5 photos: {len([person for person, image_count in people_list if image_count>=5])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People with 5+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_persons = {}\n",
    "i = 0\n",
    "\n",
    "for person,image_count in people_list:\n",
    "    if image_count >=5:\n",
    "        file_list = []\n",
    "        \n",
    "        # create new folder in selected images path\n",
    "        newpath = selection_folder + '/' + person.split('/')[-1]\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        \n",
    "        # copy / paste first 5 images to the new location\n",
    "        files = [os.path.join(person, f) for f in os.listdir(person) if os.path.isfile(os.path.join(person, f))]\n",
    "        files = files[0:5] # select first 5 images\n",
    "        for file in files:\n",
    "            filename = file.split('/')[-1]\n",
    "            shutil.copyfile(file, newpath + '/' + filename)\n",
    "            file_list.append(newpath + '/' + filename)\n",
    "            \n",
    "        selected_persons[i] = file_list\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "\n",
    "for item in selected_persons.items():\n",
    "    images = item[1]\n",
    "    \n",
    "    for i in range(len(images)-1):\n",
    "        for j in range(i+1,len(images)):\n",
    "            anchor = images[i]\n",
    "            positive = images[j]\n",
    "            \n",
    "            # choose a random negative\n",
    "            # first generate a random class rank and make sure we're not selecting the current class\n",
    "            random_class = item[0]\n",
    "            while random_class == item[0]:\n",
    "                random_class = random.randint(0, len(selected_persons)-1)\n",
    "            # selected a random image from the 5 that any of our classes has\n",
    "            random_image = random.randint(0, 4)\n",
    "            negative = selected_persons[random_class][random_image]\n",
    "            \n",
    "            triplets.append((anchor, positive, negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(filename):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels = 3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(triplets):\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(7,12))\n",
    "    axis = fig.subplots(5, 3)\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        anchor,positive,negative = triplets[40+i]\n",
    "        show(axis[i,0], preprocess_image(anchor))\n",
    "        show(axis[i,1], preprocess_image(positive))\n",
    "        show(axis[i,2], preprocess_image(negative))\n",
    "\n",
    "\n",
    "plot_images(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplets(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Inputs: a tuple of filenames\n",
    "    Output: a tuple of preprocessed images \n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=101)\n",
    "rng.shuffle(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images = [a_tuple[0] for a_tuple in triplets]\n",
    "positive_images = [a_tuple[1] for a_tuple in triplets]\n",
    "negative_images = [a_tuple[2] for a_tuple in triplets]\n",
    "\n",
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.map(preprocess_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset.take(round(image_count * 0.8))\n",
    "validation_data = dataset.skip(round(image_count * 0.8))\n",
    "\n",
    "training_data = training_data.batch(32, drop_remainder=False)\n",
    "training_data = training_data.prefetch(8)\n",
    "\n",
    "validation_data = validation_data.batch(32, drop_remainder=False)\n",
    "validation_data = validation_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_GPU = SiameseModelClass(siamese_model)\n",
    "model_on_GPU.compile(optimizer = optimizers.Adam(0.0001))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "history = model_on_GPU.fit(\n",
    "    training_data, \n",
    "    epochs=epochs, \n",
    "    validation_data = validation_data\n",
    ")\n",
    "stop = time.time()\n",
    "print(f'Training on GPU took: {(stop-start)/60} minutes')\n",
    "\n",
    "model_on_GPU.save(\"lfw.model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_GPU.save(\"lfw.model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss during training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the training was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(training_data))\n",
    "#plot_images(*sample)\n",
    "\n",
    "anchor, positive, negative = sample\n",
    "anchor_embedding, positive_embedding, negative_embedding = (\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(anchor)),\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(positive)),\n",
    "    transfer_inception_model(inputs = inception_v3.preprocess_input(negative)),\n",
    ")\n",
    "\n",
    "d1 = np. sum(np. power((anchor_embedding-positive_embedding),2))\n",
    "print(f'Anchor-positive difference = {d1}')\n",
    "\n",
    "d2 = np. sum(np. power((anchor_embedding-negative_embedding),2))\n",
    "print(f'Anchor-negative difference = {d2}')\n",
    "\n",
    "cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "print(\"Negative similarity\", negative_similarity.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "133/133 [==============================] - 12s 92ms/step - loss: 0.5031\n",
      "Loss: 0.5031144022941589\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = model_on_GPU.evaluate(validation_data)\n",
    "\n",
    "print(f'Loss: {evaluation_results}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
